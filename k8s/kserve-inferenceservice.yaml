# KServe InferenceService for model serving
# Requires KServe to be installed in your cluster
# Install: https://kserve.github.io/website/latest/admin/serverless/serverless/

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: iris-model
  annotations:
    serving.kserve.io/enable-prometheus-scraping: "true"
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 5
    imagePullSecrets:
    - name: dockerhub-secret
    containers:
    - name: kserve-container
      image: ml-model:latest  # Replace with your Docker registry image
      ports:
      - containerPort: 5000
        protocol: TCP
        name: http1
      env:
      - name: MODEL_NAME
        value: "iris-model"
      resources:
        requests:
          cpu: "250m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
      livenessProbe:
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 20
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 3
  # Optional: Add transformer for pre/post processing
  # transformer:
  #   containers:
  #   - name: transformer
  #     image: your-transformer-image:latest
